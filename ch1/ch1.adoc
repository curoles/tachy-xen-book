== Zen of Xen

=== Power of Name

If you are a fan of fantasy books, you know that a name has power.
Let us find out what name Xen means.
Xen was originally developed as virtual machine monitor for XenoServers project.
Word _xeno_ (/'zeno) originates from Greek word xenos,
meaning "alien, stranger, foreigner".
Therefore the name Xen is hinting at advanced alien technology.

Word Xen is often combined with word "hypervisor",
because Xen virtual machine monitor is a hypervisor.
Prefix _hyper_ here means "above super". 
OS kernel runs at elevated privilege level compare to applications
and is called _supervisor_ because it a) manages the applications
and b) has more priviledges than application code.
When OS kernel runs under control of Xen virtual machine monitor,
the kernal is supervised by the Xen, and Xen has higher privilege level.
The fact that Xen runs at higher privilege level than "supervisor mode" OS code
and supervises it is the origin of the term "hypervisor".

Sometimes word Xen is used just for the hypervisor,
other times it is used for all the parts involved in to
the virtualization solution:
hypervisor, OS kernel virtualization modifications,
drivers for paravirtualization, tools and etc.

=== What Is Virtualization?

Xen virtualizes the computing system as whole,
including CPU, memory, storage devices, network resources, I/O devices and etc.
The virtualization in this case is the act of creating a virtual
(rather than actual) version of a computer hardware platform.
Normally, in order to get most benifits from the virtualization,
more than one virtual hardware platform is created.
The case when a single virtual platform is created
can be considered an emulation (and it is also has its usages).

=== Hypervisor-based approach to virtualization

There are many different approaches to virtualization,
we are not going to discuss and compare them here;
if you are interested in this topic,
there are many available books and Internet resources that explain
different virtualization techniques.
Xen is using hypervisor-based approach, where a hypervisor
is a low-level virtual machine monitor that runs directly
on the physical hardware and loads first during the machine
boot process.
The hypervisor runs virtual machines (VM).
Virtual machine runs operating system (which runs applications).
Normally, an operating system is compiled for the same instruction set
as the physical machine on which the virtual systems are running.

[ditaa]
....
+------------------------------------------------+
|                                                |
| +--------------------------------------------+ |
| |                                            | |
| | +----------+  +----------+  +----------+   | |
| | | Virtual  |  | Virtual  |  | Virtual  |   | |
| | | Machine  |  | Machine  |  | Machine  |   | |
| | |  with    |  |  with    |  |  with    |   | |
| | | Admini-  |  | Guest OS |  | Guest OS |   | |
| | | strative |  |          |  |          |   | |
| | | Control  |  +----------+  +----------+   | |
| | |          |     domU          domU        | |
| | |          |                               | |
| | +----------+                               | |
| |    dom0                                    | |
| |                 Hypervisor                 | |
| | cPNK                                       | |
| +--------------------------------------------+ |
|                                                |
|                    Hardware                    |
| cBLU                                           |
+------------------------------------------------+
....

Xen terminology defines a concept of "Xen domain", which is
a specific instance of a Xen virtual machine.
Xen supports two basic types of domains:

* Xen architecture has one domain with a specially privileged
  Xen-modified kernel that is used to manage, monitor, and administer all
  other Xen virtual machines. This specially priviliged domain and
  kernel is known as _domain0_ or _dom0_.
  This kernel communicates with the hypervisor.
* Other domains are known as guest domains, unprivileged domains,
  _domainU_ or _domU_.
  It is _dom0_ that starts any _domU_.

=== CPU Virtualization

For a CPU to be completely virtualized, core running in one domain must not
affect code running in another domains.
Privileged instructions have direct access to CPU resources and can access
memory by physical address. These instructions present the biggest
problem for virtualization, as they can change CPU state and affect
all code running on that CPU.
Popek and Goldberg <<popek>> in their 1974 paper
"Formal Requirements for Virtualizable Third Generation Archotectures."
devide critical for virtualization instructions into three categories:

* Privileged instructions that execute in a privilege mode.
  They have to trap if executed outside of their privilege mode.
  In a virtualized system, the trap handler makes sure that
  the effect of the instruction is contained inside the domain.
* Control sensitive instructions that change global CPU state
  by communicating with devices, changing global configuration registers,
  updating virtual to physical memory mappings.
* Behavior sensitive instructions that behave differently depending
  on the configuration.

In order for an architecture to be virtualizable, all control sensitive
instructions must be trappable privileged instructions.

=== Memory Virtualization

Memory virtualization is relatively straightforward.
Each physical memory page is assigned to one domain,
this way physical memory is spit between domains;
the difficulty is
due to the fact that OS kernel virtual-to-physical translation
operates with interemidiate "psedo" physical address that needs to be
translated into "real" physical address, called machine address.
Every priviliged instruction that accesses physical memory or
changes virtual to physical mapping must be trapped.

.Memory virtualization performance critical HW support
* HW support for 2 levels page-tables walk: virtual-to-physical
  and physical-to-machine.
* Tagged TLB that allows flushing per domain, without flushing all TLB entries.

=== I/O Virtualization

With devices, other than CPU and memory, there are two main reasons
why virtualization is difficult:

* Most devices are not designed with virtualization in mind.
* For some devices it is not obvious what virtualization means for them
  or how these devices can support virtualization.

For example, most of the time it is easier to assign keyboard, mouse
and display to one domain, instead of sharing it.
On other hand, storage devices and network devices most of the time
are shared.

Storage devices are usually virtualized similar to memory by partitioning
the physical space between domains.

One well known problem is when a device uses DMA without
support from _Input/Output Memory Management Unit_ (IOMMU).
When unmodified OS is unaware it is running not on actual hardware
platform but on virtual platform, DMA will use hypervisor-provided
physical address instead of machine address.

Here is a list of suggestions for choosing existing devices for your system
or when new device is to be designed:

* Make sure the device has _Input/Output Memory Management Unit_ (IOMMU).
* Make sure the whole device state can be saved and later restored.
* For security critical devices, make sure there is a way to protect
  information in case multiple domains share this device.

=== Paravirtualization

The paravirtualization approach assumes that a guest system _knows_ that it
runs on top of a hypervisor. Because of that knoweledge it becomes possible:

1. to compile OS kernel without using problematic instructions that do not trap
   into the hypervisor, intead use some direct methods to communuicate with
   the hypervisor;
2. run inside less priveledged level and do not use any priveledged instructions,
   instead delegate all work to the hypevisor by means of _hypercalls_;
3. simplify a view at the system, do not implement device drivers for any real hardware,
   instead delegate administrative domain dom0 to talk to the real hardware and
   use simple drivers to communicate with dom0 abstract devices.

Note that in the past when x86 did not have enough support for the full hardware-assisted
virtualization, the paravirtualization was the only sensible solution
providing reasonable performance. However, it does not mean that the paravirtualization
is not used now days; due to its simplicity compare to HW/SW solutions and the fact
that in many cases paravirtualizartion still provides better performance,
the paravirtualization is widely used when customizing OS is possible.


=== Hardware-Assisted Virtualization

The hardware assisted virtualization, often referred to as HVM (Hardware Virtual Machine),
allows running of unmodified operating systems and relies on HW support of virtualization.

=== Types of virtualization

The are many definitions and terminology involved when we look at different
types of virtualization, so many that it is confusing sometimes.
First of all, there are hypervisor Type-1 and Type-2.

From here on we will use Tachyum notation PLn of the privilege levels, where
n=0 is most priveleged level, n=1 is hypervisor level, n=2 is kernel level
and n=3 is user level.

Hypervisor Type-1 is native or bare-metal hypervisor.
These hypervisors run directly on the host's hardware
to control the hardware and to manage guest operating systems.

[ditaa]
....
  +----------+ +----------+ +----------+
  |   Apps   | |   Apps   | |   Apps   |  PL3
  +----------+ +----------+ +----------+

+----------------------------------------+

  +----------+ +----------+ +----------+
  | Admin OS | | Guest OS | | Guest OS |  PL2
  +----------+ +----------+ +----------+

+----------------------------------------+

  +------------------------------------+
  |          Hypervisor                |  PL1
  +------------------------------------+

+----------------------------------------+

  +------------------------------------+
  |           Hardware                 |
  +------------------------------------+
....

Hypervisor Type-2 is hosted hypervisor.
These hypervisors run on a conventional operating system
just as other computer programs do.
A guest operating system runs as a process on the host.
Type-2 hypervisors abstract guest operating systems from the host operating system.

[ditaa]
....
  +----------+ +----------+ +----------+
  |   Apps   | |   Apps   | |   Apps   |  PL3
  +----------+ +----------+ +----------+

+----------------------------------------+
               +----------+ +----------+
               | Guest OS | | Guest OS |
               +----------+ +----------+

  +------------------------------------+
  |                                    |
  |  Host OS                           |  PL2
  |            +-----------------------+
  |            | +-----------------------+
  | Hypervisor |
  |            |
  +------------+                        PL1

+----------------------------------------+
  +------------------------------------+
  |           Hardware                 |
  +------------------------------------+
....

The line between Type-1 and Type-2 sometime is not clear.
For instance, Linux's Kernel-based Virtual Machine (KVM) is a) kernel module,
b) uses _lowvisor_ that can run at PL1, c) the whole host OS can
run in PL1 on ARM systems with Virtualization Host Extension support.

Xen is hypervisor Type-1.

Next distinction is by degree of HW support used by virtualization and
how much is paravirtualized. Remember we talked about pure paravirtualization (PV)
approach and hardware assisted HVM approach.
In Xen there are many hybrid solutions in between these two.

Looking at the history of Xen development can shed some light on the
origin of different hybrid solutions.

.Guest types evolutuion for Xen, wiki.xenproject.org.
image::https://wiki.xenproject.org/images/b/b1/GuestModes.png[]

.Overview of the various virtualization modes implemented in Xen, wiki.xenproject.org.
image::https://wiki.xenproject.org/images/f/f2/XenModes.png[]

=== The role of dom0

TODO

=== Unprivileged domains

TODO

https://wiki.xenproject.org/wiki/Xen_Project_Software_Overview,
I/O Virtualization in Xen. No HVM without QEMU???

.I/O Virtualization using the split driver model .
image::https://wiki.xenproject.org/images/a/ae/IOVirt_PV.png[]

.I/O Virtualization using QEMU user space back-end drivers.
image::https://wiki.xenproject.org/images/3/3c/IOVirt_QEMU.png[]



=== History Of Problems with Virtualization

While designing a new CPU the architects should be aware of the problems other CPU architectures
had or still have to support virtualization by Xen.
The understading of past and existing problems is crutial in designing
a CPU that can support virtualization efficiently, when overhead of running
the hypervisor is practically unnoticeable.

* One well known problem with x86 was that some privileged instructions did not
  trap when they were executed with unsufficient privileges failing silently.
  Some virtualizers monitored instruction stream and patched those misbehaving
  instructions, practically performing binary translation, which caused
  significant degradation of performance.
* In past many architechtures did not have a protection level designed specifically
  for a hypervisor even when they have several protection levels.
* In case of paravirtualization, an absense of a special instructions in the ISA
  to be used for fast hypecall to the hypervisor is very critical for the performance.
* It used to be on x86 that booting 32-bit domain0 dictated all other domainU kernels
  to be 32-bit, similar for 64-bit domain0.
  New CPU architecture should allow different meaningful combinations of kernels, including
  bitness and endianess.
* TODO DMA, absence of IOMMU
* TODO Need to use QEMU for full system virtualization FVM

=== CPU And System Support For Virtualization

.Required functinality
* Ability to bind a virtual machine to a specific CPU on the host system.
  It helps to solve performance problems of a virtual machine under heavy load.
* No limitation for the size of virtual address space available for OS kernels
  running inside virtual machine.
* Each guest OS may create high network traffic, multiple guests can easily
  overload the capabilities of a single network interface.
  The system must support multiple network interfaces with high bandwidth.
* The system must support high IO traffic as guests may run storage intensive
  applications, such as database applications.
* The system must satisfy all requirements of Trusted Computing for the virtualized
  environment when multiple operating systems are simultaneously running on
  a single platform (with a single TPM device).
  Including the secure migration of the TPM state from one physical system
  to another when domainU guests are migrated from one system to another.
* Security concerns due to potential information leaks when
  instructions executed speculatively.
* Ability to save virtual machine state and migrate it easily to another machine.
* Ability to debug code in any privilige mode (protection ring).
* <<dall>> Ability to run Host OS at hypervisor privilege level without paying
  high price for the swithing levels, levels should have its own copy
  of all special registers to avoid saving and restoring state.
  OS running at PL1 should be able to communicate with PL3 user space code;
  requires handling exceptions from PL3 directly to PL1.
* 2 stage memory translation
* <<dall>> Hypervisor software running in PL1(HYP) should be able
  to completely disable the stage 2 translations when running
  the hypervisor OS kernel (KVM) or dom0(Xen),
  giving it full access to all physical memory on the system,
  and conversely enable stage 2 translations when running VM
  kernels to limit VMs to manage memory allocated to them.

=== TODO STUDY

Rapid Virtualization Indexing (RVI)::
Helps accelerate the performance of many virtualized applications
by enabling hardware-based VM memory management

AMD-V Extended Migration::
Helps virtualization software with live migrations of VMs between all available AMD Opteron processor generations

check references:
https://en.wikipedia.org/wiki/X86_virtualization#AMD_virtualization_(AMD-V)

https://lwn.net/Articles/182080/ says: There is also a cache called an IOTLB which improves performance.
In the AMD IOMMU there is optional support for IOTLBs.

https://www.starlab.io/blog/how-the-xen-hypervisor-supports-cpu-virtualization-on-arm

<<dall>> What if we run dom0 at PL1?

arm64: Virtualization Host Extension support,
https://lwn.net/Articles/674533/

https://www.embedded.com/understanding-virtualization-facilities-in-the-armv8-processor-architecture/,
the second translation table for the EL2 level, TTBR1_EL2, was added as a part of VM host extensions so that the hypervisors of Type 2 would have its own translation

check sMMUv3.1, ARM System Memory Management Unit Architecture Specification SMMU architecture version 3.0 and version 3.1

interesting usage of time diagrams: https://genode.org/documentation/articles/arm_virtualization

http://www.cs.columbia.edu/~cdall/pubs/isca2016-dall.pdf

ARM provides interrupt virtualization through a set of virtualization extensions to the ARM Generic Interrupt Controller (GIC) architecture, which allows a hypervisor to program the GIC to inject virtual interrupts to VMs, which VMs can acknowledge and complete without trapping to the hypervisor.

http://www.linux-kvm.org/images/7/79/03x09-Aspen-Andre_Przywara-ARM_Interrupt_Virtualization.pdf